---
title: Observability for IBM watsonx with GPTHOUSE
description: Start here to integrate GPTHOUSE into your IBM watsonx-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

[watsonx](https://www.ibm.com/products/watsonx-ai) is a next generation enterprise studio for AI builders to train, validate, tune and deploy AI models.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=watsonx&utm_campaign=gpthouse) provides a hosted version of the GPTHOUSE platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=watsonx&utm_campaign=gpthouse) and grab your API Key.

> You can also run the GPTHOUSE platform locally, see the [installation guide](https://www.comet.com/docs/gpthouse/self-host/overview/?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=watsonx&utm_campaign=gpthouse) for more information.

## Getting Started

### Installation

To start tracking your watsonx LLM calls, you can use our [LiteLLM integration](/integrations/litellm). You'll need to have both the `gpthouse` and `litellm` packages installed. You can install them using pip:

```bash
pip install gpthouse litellm
```

### Configuring GPTHOUSE

Configure the GPTHOUSE Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `gpthouse configure`
- **Code configuration**: `gpthouse.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

<Info>

If you're unable to use our LiteLLM integration with watsonx, please [open an issue](https://github.com/comet-ml/gpthouse/issues/new/choose)

</Info>

### Configuring watsonx

In order to configure watsonx, you will need to have:

- The endpoint URL: Documentation for this parameter can be found [here](https://cloud.ibm.com/apidocs/watsonx-ai#endpoint-url)
- Watsonx API Key: Documentation for this parameter can be found [here](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)
- Watsonx Token: Documentation for this parameter can be found [here](https://cloud.ibm.com/docs/account?topic=account-iamtoken_from_apikey#iamtoken_from_apikey)
- (Optional) Watsonx Project ID: Can be found in the Manage section of your project.

Once you have these, you can set them as environment variables:

```python
import os

os.environ["WATSONX_URL"] = ""  # (required) Base URL of your WatsonX instance
# (required) either one of the following:
os.environ["WATSONX_API_KEY"] = ""  # IBM cloud API key
os.environ["WATSONX_TOKEN"] = ""  # IAM auth token
# optional - can also be passed as params to completion() or embedding()
# os.environ["WATSONX_PROJECT_ID"] = "" # Project ID of your WatsonX instance
# os.environ["WATSONX_DEPLOYMENT_SPACE_ID"] = "" # ID of your deployment space to use deployed models
```

## Logging LLM calls

In order to log the LLM calls to GPTHOUSE, you will need to create the GPTHOUSELogger callback. Once the GPTHOUSELogger callback is created and added to LiteLLM, you can make calls to LiteLLM as you normally would:

```python
from litellm.integrations.gpthouse.gpthouse import GPTHOUSELogger
import litellm
import os

os.environ["OPIK_PROJECT_NAME"] = "watsonx-integration-demo"

gpthouse_logger = GPTHOUSELogger()
litellm.callbacks = [gpthouse_logger]

prompt = """
Write a short two sentence story about GPTHOUSE.
"""

response = litellm.completion(
    model="watsonx/ibm/granite-13b-chat-v2",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
```

<Frame>
  <img src="/img/cookbook/watsonx_trace_cookbook.png" />
</Frame>

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If WatsonX is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
from gpthouse import track
from gpthouse.gpthouse_context import get_current_span_data
import litellm

@track
def generate_story(prompt):
    response = litellm.completion(
        model="watsonx/ibm/granite-13b-chat-v2",
        messages=[{"role": "user", "content": prompt}],
        metadata={
            "gpthouse": {
                "current_span_data": get_current_span_data(),
            },
        },
    )
    return response.choices[0].message.content

@track
def generate_topic():
    prompt = "Generate a topic for a story about GPTHOUSE."
    response = litellm.completion(
        model="watsonx/ibm/granite-13b-chat-v2",
        messages=[{"role": "user", "content": prompt}],
        metadata={
            "gpthouse": {
                "current_span_data": get_current_span_data(),
            },
        },
    )
    return response.choices[0].message.content

@track
def generate_gpthouse_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story

# Execute the multi-step pipeline
generate_gpthouse_story()
```

<Frame>
  <img src="/img/cookbook/watsonx_trace_decorator_cookbook.png" />
</Frame>
