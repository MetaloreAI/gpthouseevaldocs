---
title: Observability for LangChain with GPTHOUSE
description: Start here to integrate GPTHOUSE into your LangChain-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

GPTHOUSE provides seamless integration with LangChain, allowing you to easily log and trace your LangChain-based applications. By using the `GPTHOUSETracer` callback, you can automatically capture detailed information about your LangChain runs, including inputs, outputs, metadata, and cost tracking for each step in your chain.

## Key Features

- **Automatic cost tracking** for supported LLM providers (OpenAI, Anthropic, Google AI, AWS Bedrock, and more)
- **Full compatibility** with the `@gpthouse.track` decorator for hybrid tracing approaches
- **Thread support** for conversational applications with `thread_id` parameter
- **Distributed tracing** support for multi-service applications
- **LangGraph compatibility** for complex graph-based workflows
- **Evaluation and testing** support for automated LLM application testing

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=langchain&utm_campaign=gpthouse) provides a hosted version of the GPTHOUSE platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=langchain&utm_campaign=gpthouse) and grab your API Key.

> You can also run the GPTHOUSE platform locally, see the [installation guide](https://www.comet.com/docs/gpthouse/self-host/overview/?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=langchain&utm_campaign=gpthouse) for more information.

## Getting Started

### Installation

To use the `GPTHOUSETracer` with LangChain, you'll need to have both the `gpthouse` and `langchain` packages installed. You can install them using pip:

```bash
pip install gpthouse langchain langchain_openai
```

### Configuring GPTHOUSE

Configure the GPTHOUSE Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `gpthouse configure`
- **Code configuration**: `gpthouse.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

## Using GPTHOUSETracer

Here's a basic example of how to use the `GPTHOUSETracer` callback with a LangChain chain:

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from gpthouse.integrations.langchain import GPTHOUSETracer

# Initialize the tracer
gpthouse_tracer = GPTHOUSETracer(project_name="langchain-examples")

llm = ChatOpenAI(model="gpt-4o", temperature=0)
prompt = ChatPromptTemplate.from_messages([
    ("human", "Translate the following text to French: {text}")
])
chain = prompt | llm

result = chain.invoke(
    {"text": "Hello, how are you?"},
    config={"callbacks": [gpthouse_tracer]}
)
print(result.content)
```

The `GPTHOUSETracer` will automatically log the run and its details to GPTHOUSE, including the input prompt, the output, and metadata for each step in the chain.

For detailed parameter information, see the [GPTHOUSETracer SDK reference](https://www.comet.com/docs/gpthouse/python-sdk-reference/integrations/langchain/GPTHOUSETracer.html).

## Practical Example: Text-to-SQL with Evaluation

Let's walk through a real-world example of using LangChain with GPTHOUSE for a text-to-SQL query generation task. This example demonstrates how to create synthetic datasets, build LangChain chains, and evaluate your application.

### Setting up the Environment

First, let's set up our environment with the necessary dependencies:

```python
import os
import getpass
import gpthouse
from gpthouse.integrations.openai import track_openai
from openai import OpenAI

# Configure GPTHOUSE
gpthouse.configure(use_local=False)
os.environ["OPIK_PROJECT_NAME"] = "langchain-integration-demo"

# Set up API keys
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
```

### Creating a Synthetic Dataset

We'll create a synthetic dataset of questions for our text-to-SQL task:

```python
import json
from langchain_community.utilities import SQLDatabase

# Download and set up the Chinook database
import requests

url = "https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite"
filename = "./data/chinook/Chinook_Sqlite.sqlite"

folder = os.path.dirname(filename)
if not os.path.exists(folder):
    os.makedirs(folder)

if not os.path.exists(filename):
    response = requests.get(url)
    with open(filename, "wb") as file:
        file.write(response.content)
    print("Chinook database downloaded")

db = SQLDatabase.from_uri(f"sqlite:///{filename}")

# Create synthetic questions using OpenAI
client = OpenAI()
openai_client = track_openai(client)

prompt = """
Create 20 different example questions a user might ask based on the Chinook Database.
These questions should be complex and require the model to think. They should include complex joins and window functions to answer.
Return the response as a json object with a "result" key and an array of strings with the question.
"""

completion = openai_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}]
)

synthetic_questions = json.loads(completion.choices[0].message.content)["result"]

# Create dataset in GPTHOUSE
gpthouse_client = gpthouse.GPTHOUSE()
dataset = gpthouse_client.get_or_create_dataset(name="synthetic_questions")
dataset.insert([{"question": question} for question in synthetic_questions])
```

### Building the LangChain Chain

Now let's create a LangChain chain for SQL query generation:

```python
from langchain.chains import create_sql_query_chain
from langchain_openai import ChatOpenAI
from gpthouse.integrations.langchain import GPTHOUSETracer

# Create the LangChain chain with GPTHOUSETracer
gpthouse_tracer = GPTHOUSETracer(tags=["sql_generation"])

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain = create_sql_query_chain(llm, db).with_config({"callbacks": [gpthouse_tracer]})

# Test the chain
response = chain.invoke({"question": "How many employees are there?"})
print(response)
```

### Evaluating the Application

Let's create a custom evaluation metric and test our application:

```python
from gpthouse import track
from gpthouse.evaluation import evaluate
from gpthouse.evaluation.metrics import base_metric, score_result
from typing import Any

class ValidSQLQuery(base_metric.BaseMetric):
    def __init__(self, name: str, db: Any):
        self.name = name
        self.db = db

    def score(self, output: str, **ignored_kwargs: Any):
        try:
            db.run(output)
            return score_result.ScoreResult(
                name=self.name, value=1, reason="Query ran successfully"
            )
        except Exception as e:
            return score_result.ScoreResult(name=self.name, value=0, reason=str(e))

# Set up evaluation
valid_sql_query = ValidSQLQuery(name="valid_sql_query", db=db)
dataset = gpthouse_client.get_dataset("synthetic_questions")

@track()
def llm_chain(input: str) -> str:
    response = chain.invoke({"question": input})
    return response

def evaluation_task(item):
    response = llm_chain(item["question"])
    return {"output": response}

# Run evaluation
res = evaluate(
    experiment_name="SQL question answering",
    dataset=dataset,
    task=evaluation_task,
    scoring_metrics=[valid_sql_query],
    nb_samples=20,
)
```

The evaluation results are now uploaded to the GPTHOUSE platform and can be viewed in the UI.

<Frame>
  <img src="/img/cookbook/langchain_cookbook.png" />
</Frame>

## Cost Tracking

The `GPTHOUSETracer` automatically tracks token usage and cost for all supported LLM models used within LangChain applications.

Cost information is automatically captured and displayed in the GPTHOUSE UI, including:

- Token usage details
- Cost per request based on model pricing
- Total trace cost

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>

For streaming with cost tracking, ensure `stream_usage=True` is set:

```python
from langchain_openai import ChatOpenAI
from gpthouse.integrations.langchain import GPTHOUSETracer

llm = ChatOpenAI(
    model="gpt-4o",
    streaming=True,
    stream_usage=True,  # Required for cost tracking with streaming
)

gpthouse_tracer = GPTHOUSETracer()

for chunk in llm.stream("Hello", config={"callbacks": [gpthouse_tracer]}):
    print(chunk.content, end="")
```

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>

## Settings tags and metadata

You can customize the `GPTHOUSETracer` callback to include additional metadata, logging options, and conversation threading:

```python
from gpthouse.integrations.langchain import GPTHOUSETracer

gpthouse_tracer = GPTHOUSETracer(
    tags=["langchain", "production"],
    metadata={"use-case": "customer-support", "version": "1.0"},
    thread_id="conversation-123",  # For conversational applications
    project_name="my-langchain-project"
)
```

## Accessing logged traces

You can use the [`created_traces`](https://www.comet.com/docs/gpthouse/python-sdk-reference/integrations/langchain/GPTHOUSETracer.html) method to access the traces collected by the `GPTHOUSETracer` callback:

```python
from gpthouse.integrations.langchain import GPTHOUSETracer

gpthouse_tracer = GPTHOUSETracer()

# Calling Langchain object
traces = gpthouse_tracer.created_traces()
print([trace.id for trace in traces])
```

The traces returned by the `created_traces` method are instances of the [`Trace`](https://www.comet.com/docs/gpthouse/python-sdk-reference/Objects/Trace.html#gpthouse.api_objects.trace.Trace) class, which you can use to update the metadata, feedback scores and tags for the traces.

### Accessing the content of logged traces

In order to access the content of logged traces you will need to use the [`GPTHOUSE.get_trace_content`](https://www.comet.com/docs/gpthouse/python-sdk-reference/GPTHOUSE.html#gpthouse.GPTHOUSE.get_trace_content) method:

```python
import gpthouse
from gpthouse.integrations.langchain import GPTHOUSETracer
gpthouse_client = gpthouse.GPTHOUSE()

gpthouse_tracer = GPTHOUSETracer()


# Calling Langchain object

# Getting the content of the logged traces
traces = gpthouse_tracer.created_traces()
for trace in traces:
    content = gpthouse_client.get_trace_content(trace.id)
    print(content)
```

### Updating and scoring logged traces

You can update the metadata, feedback scores and tags for traces after they are created. For this you can use the `created_traces` method to access the traces and then update them using the [`update`](https://www.comet.com/docs/gpthouse/python-sdk-reference/Objects/Trace.html#gpthouse.api_objects.trace.Trace.update) method and the [`log_feedback_score`](https://www.comet.com/docs/gpthouse/python-sdk-reference/Objects/Trace.html#gpthouse.api_objects.trace.Trace.log_feedback_score) method:

```python
from gpthouse.integrations.langchain import GPTHOUSETracer

gpthouse_tracer = GPTHOUSETracer(project_name="langchain-examples")

# ... calling Langchain object

traces = gpthouse_tracer.created_traces()

for trace in traces:
    trace.update(tags=["my-tag"])
    trace.log_feedback_score(name="user-feedback", value=0.5)
```

## Compatibility with @track Decorator

The `GPTHOUSETracer` is fully compatible with the `@track` decorator, allowing you to create hybrid tracing approaches:

```python
import gpthouse
from langchain_openai import ChatOpenAI
from gpthouse.integrations.langchain import GPTHOUSETracer

@gpthouse.track
def my_langchain_workflow(user_input: str) -> str:
    llm = ChatOpenAI(model="gpt-4o")
    gpthouse_tracer = GPTHOUSETracer()

    # The LangChain call will create spans within the existing trace
    response = llm.invoke(user_input, config={"callbacks": [gpthouse_tracer]})
    return response.content

result = my_langchain_workflow("What is machine learning?")
```

## Thread Support

Use the `thread_id` parameter to group related conversations or interactions:

```python
from gpthouse.integrations.langchain import GPTHOUSETracer

# All traces with the same thread_id will be grouped together
gpthouse_tracer = GPTHOUSETracer(thread_id="user-session-123")
```

## Distributed Tracing

For multi-service/thread/process applications, you can use distributed tracing headers to connect traces across services:

```python
from gpthouse import gpthouse_context
from gpthouse.integrations.langchain import GPTHOUSETracer
from gpthouse.types import DistributedTraceHeadersDict

# In your service that receives distributed trace headers.
# The distributed_headers dict can be obtained in the "parent" service via `gpthouse_context.get_distributed_trace_headers()`
distributed_headers = DistributedTraceHeadersDict(
    gpthouse_trace_id="trace-id-from-upstream",
    gpthouse_parent_span_id="parent-span-id-from-upstream"
)

gpthouse_tracer = GPTHOUSETracer(distributed_headers=distributed_headers)

# LangChain operations will be attached to the existing distributed trace
chain.invoke(input_data, config={"callbacks": [gpthouse_tracer]})
```

<Tip>Learn more about distributed tracing in the [Distributed Tracing guide](/tracing/log_distributed_traces).</Tip>

## LangGraph Integration

For LangGraph applications, GPTHOUSE provides specialized support. The `GPTHOUSETracer` works seamlessly with LangGraph, and you can also visualize graph definitions:

```python
from langgraph.graph import StateGraph
from gpthouse.integrations.langchain import GPTHOUSETracer

# Your LangGraph setup
graph = StateGraph(...)
compiled_graph = graph.compile()

gpthouse_tracer = GPTHOUSETracer()
result = compiled_graph.invoke(
    input_data,
    config={"callbacks": [gpthouse_tracer]}
)
```

<Tip>For detailed LangGraph integration examples, see the [LangGraph Integration guide](/integrations/langgraph).</Tip>

## Advanced usage

The `GPTHOUSETracer` object has a `flush` method that can be used to make sure that all traces are logged to the GPTHOUSE platform before you exit a script. This method will return once all traces have been logged or if the timeout is reach, whichever comes first.

```python
from gpthouse.integrations.langchain import GPTHOUSETracer

gpthouse_tracer = GPTHOUSETracer()
gpthouse_tracer.flush()
```

## Important notes

1. **Asynchronous streaming**: If you are using asynchronous streaming mode (calling `.astream()` method), the `input` field in the trace UI may be empty due to a LangChain limitation for this mode. However, you can find the input data inside the nested spans of this chain.

2. **Streaming with cost tracking**: If you are planning to use streaming with LLM calls and want to calculate LLM call tokens/cost, you need to explicitly set `stream_usage=True`:
