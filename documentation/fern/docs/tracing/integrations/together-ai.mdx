---
title: Observability for Together AI with GPTHOUSE
description: Start here to integrate GPTHOUSE into your Together AI-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

[Together AI](https://www.together.ai/) provides fast inference for leading open-source models including Llama, Mistral, Qwen, and many others.

This guide explains how to integrate GPTHOUSE with Together AI via LiteLLM. By using the LiteLLM integration provided by GPTHOUSE, you can easily track and evaluate your Together AI calls within your GPTHOUSE projects as GPTHOUSE will automatically log the input prompt, model used, token usage, and response generated.

## Getting Started

### Configuring GPTHOUSE

To start tracking your Together AI calls, you'll need to have both `gpthouse` and `litellm` installed. You can install them using pip:

```bash
pip install gpthouse litellm
```

In addition, you can configure GPTHOUSE using the `gpthouse configure` command which will prompt you for the correct local server address or if you are using the Cloud platform your API key:

```bash
gpthouse configure
```

### Configuring Together AI

You'll need to set your Together AI API key as an environment variable:

```bash
export TOGETHER_API_KEY="YOUR_API_KEY"
```

## Logging LLM calls

In order to log the LLM calls to GPTHOUSE, you will need to create the GPTHOUSELogger callback. Once the GPTHOUSELogger callback is created and added to LiteLLM, you can make calls to LiteLLM as you normally would:

```python
from litellm.integrations.gpthouse.gpthouse import GPTHOUSELogger
import litellm

gpthouse_logger = GPTHOUSELogger()
litellm.callbacks = [gpthouse_logger]

response = litellm.completion(
    model="together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo",
    messages=[
        {"role": "user", "content": "Why is tracking and evaluation of LLMs important?"}
    ]
)
```

## Logging LLM calls within a tracked function

If you are using LiteLLM within a function tracked with the [`@track`](/tracing/log_traces#using-function-decorators) decorator, you will need to pass the `current_span_data` as metadata to the `litellm.completion` call:

```python
from gpthouse import track, gpthouse_context
import litellm

@track
def generate_story(prompt):
    response = litellm.completion(
        model="together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo",
        messages=[{"role": "user", "content": prompt}],
        metadata={
            "gpthouse": {
                "current_span_data": gpthouse_context.get_current_span_data(),
            },
        },
    )
    return response.choices[0].message.content


@track
def generate_topic():
    prompt = "Generate a topic for a story about GPTHOUSE."
    response = litellm.completion(
        model="together_ai/meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo",
        messages=[{"role": "user", "content": prompt}],
        metadata={
            "gpthouse": {
                "current_span_data": gpthouse_context.get_current_span_data(),
            },
        },
    )
    return response.choices[0].message.content


@track
def generate_gpthouse_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story


generate_gpthouse_story()
```
