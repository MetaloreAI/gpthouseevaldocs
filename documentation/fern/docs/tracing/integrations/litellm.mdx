---
title: Observability for LiteLLM with GPTHOUSE
description: Start here to integrate GPTHOUSE into your LiteLLM-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

[LiteLLM](https://github.com/BerriAI/litellm) allows you to call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]. There are two main ways to use LiteLLM:

1. Using the [LiteLLM Python SDK](https://docs.litellm.ai/docs/#litellm-python-sdk)
2. Using the [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/#litellm-proxy-server-llm-gateway)

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=litellm&utm_campaign=gpthouse) provides a hosted version of the GPTHOUSE platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=litellm&utm_campaign=gpthouse) and grab your API Key.

> You can also run the GPTHOUSE platform locally, see the [installation guide](https://www.comet.com/docs/gpthouse/self-host/overview/?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=litellm&utm_campaign=gpthouse) for more information.

## Getting Started

### Installation

First, ensure you have both `gpthouse` and `litellm` packages installed:

```bash
pip install gpthouse litellm
```

### Configuring GPTHOUSE

Configure the GPTHOUSE Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `gpthouse configure`
- **Code configuration**: `gpthouse.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring LiteLLM

In order to use LiteLLM, you will need to configure your LLM provider API keys. For this example, we'll use OpenAI. You can [find or create your API keys in these pages](https://platform.openai.com/settings/organization/api-keys):

You can set them as environment variables:

```bash
export OPENAI_API_KEY="YOUR_API_KEY"
```

Or set them programmatically:

```python
import os
import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
```

## Using GPTHOUSE with the LiteLLM Python SDK

### Logging LLM calls

In order to log the LLM calls to GPTHOUSE, you will need to create the GPTHOUSELogger callback. Once the GPTHOUSELogger callback is created and added to LiteLLM, you can make calls to LiteLLM as you normally would:

```python
from litellm.integrations.gpthouse.gpthouse import GPTHOUSELogger
import litellm
import os

# Set project name for better organization
os.environ["GPTHOUSE_PROJECT_NAME"] = "litellm-integration-demo"

gpthouse_logger = GPTHOUSELogger()
litellm.callbacks = [gpthouse_logger]

response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Why is tracking and evaluation of LLMs important?"}
    ]
)

print(response.choices[0].message.content)
```

<Frame>
  <img src="/img/cookbook/litellm_cookbook.png" />
</Frame>

### Logging LLM calls within a tracked function

If you are using LiteLLM within a function tracked with the [`@track`](/tracing/log_traces#using-function-decorators) decorator, you will need to pass the `current_span_data` as metadata to the `litellm.completion` call:

```python
from gpthouse import track
from gpthouse.gpthouse_context import get_current_span_data
from litellm.integrations.gpthouse.gpthouse import GPTHOUSELogger
import litellm

gpthouse_logger = GPTHOUSELogger()
litellm.callbacks = [gpthouse_logger]

@track
def streaming_function(input):
    messages = [{"role": "user", "content": input}]
    response = litellm.completion(
        model="gpt-3.5-turbo",
        messages=messages,
        metadata = {
            "gpthouse": {
                "current_span_data": get_current_span_data(),
                "tags": ["streaming-test"],
            },
        }
    )
    return response

response = streaming_function("Why is tracking and evaluation of LLMs important?")
chunks = list(response)
```

## Using GPTHOUSE with the LiteLLM Proxy Server

<Info>
  **GPTHOUSE Agent Optimizer & LiteLLM**: Beyond tracing, the GPTHOUSE Agent Optimizer SDK also leverages LiteLLM for comprehensive model support within its optimization algorithms. This allows you to use a wide range of LLMs (including local ones) for prompt optimization tasks. Learn more on the [LiteLLM Support for Optimizers page](/agent_optimization/gpthouse_optimizer/litellm_support).
</Info>

### Configuring the LiteLLM Proxy Server

In order to configure the GPTHOUSE logging, you will need to update the `litellm_settings` section in the LiteLLM `config.yaml` config file:

```yaml
model_list:
  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
litellm_settings:
  success_callback: ["gpthouse"]
```

You can now start the LiteLLM Proxy Server and all LLM calls will be logged to GPTHOUSE:

```bash
litellm --config config.yaml
```

### Using the LiteLLM Proxy Server

Each API call made to the LiteLLM Proxy server will now be logged to GPTHOUSE:

```bash
curl -X POST http://localhost:4000/v1/chat/completions -H 'Authorization: Bearer sk-1234' -H "Content-Type: application/json" -d '{
    "model": "gpt-4o",
    "messages": [
        {
            "role": "user",
            "content": "Hello!"
        }
    ]
}'
```
