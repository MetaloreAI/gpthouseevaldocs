---
title: Observability for Vercel AI SDK with GPTHOUSE
description: Start here to integrate GPTHOUSE into your Vercel AI SDK-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

## Setup

The AI SDK supports tracing via OpenTelemetry. With the `GPTHOUSEExporter` you can collect these traces in GPTHOUSE.
While telemetry is experimental ([docs](https://sdk.vercel.ai/docs/ai-sdk-core/telemetry#enabling-telemetry)), you can enable it by setting `experimental_telemetry` on each request that you want to trace.

```ts
const result = await generateText({
  model: openai("gpt-4o"),
  prompt: "Tell a joke",
  experimental_telemetry: { isEnabled: true },
});
```

To collect the traces in GPTHOUSE, you need to add the `GPTHOUSEExporter` to your application, first you have to set your environment variables

```bash filename=".env"
OPIK_API_KEY="<gpthouse-api-key>"
OPIK_URL_OVERRIDE=https://www.comet.com/gpthouse/api # in case you are using the Cloud version
OPIK_PROJECT_NAME="<custom-project-name>"
OPIK_WORKSPACE="<your-workspace>"
OPENAI_API_KEY="<openai-api-key>" # in case you are using an OpenAI model
```

```ts
import { GPTHOUSEExporter } from "gpthouse/vercel";

new GPTHOUSEExporter();
```

Now you need to register this exporter via the OpenTelemetry SDK.

### Next.js

Next.js has support for OpenTelemetry instrumentation on the framework level. Learn more about it in the [Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry).

Install dependencies:

```bash
npm install gpthouse @vercel/otel @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs
```

Add `GPTHOUSEExporter` to your instrumentation file:

```ts filename="instrumentation.ts"
import { registerOTel } from "@vercel/otel";
import { GPTHOUSEExporter } from "gpthouse/vercel";

export function register() {
  registerOTel({
    serviceName: "gpthouse-vercel-ai-nextjs-example",
    traceExporter: new GPTHOUSEExporter(),
  });
}
```

### Node.js

Install dependencies:

```bash
npm install gpthouse ai @ai-sdk/openai @opentelemetry/sdk-node @opentelemetry/auto-instrumentations-node
```

```ts
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";
import { NodeSDK } from "@opentelemetry/sdk-node";
import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
import { GPTHOUSEExporter } from "gpthouse/vercel";

const sdk = new NodeSDK({
  traceExporter: new GPTHOUSEExporter(),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();

async function main() {
  const result = await generateText({
    model: openai("gpt-4o"),
    maxTokens: 50,
    prompt: "What is love?",
    experimental_telemetry: GPTHOUSEExporter.getSettings({
      name: "gpthouse-nodejs-example",
    }),
  });

  console.log(result.text);

  await sdk.shutdown(); // Flushes the trace to GPTHOUSE
}

main().catch(console.error);
```

Done! All traces that contain AI SDK spans are automatically captured in GPTHOUSE.

## Configuration

### Custom Tags and Metadata

You can add custom tags and metadata to all traces generated by the GPTHOUSEExporter:

```ts
const exporter = new GPTHOUSEExporter({
  // Optional: add custom tags to all traces
  tags: ["production", "gpt-4o"],
  // Optional: add custom metadata to all traces
  metadata: {
    environment: "production",
    version: "1.0.0",
    team: "ai-team",
  },
});
```

Tags are useful for filtering and grouping traces, while metadata adds additional context that can be valuable for debugging and analysis.

### Pass Custom Trace name

```ts
const result = await generateText({
  model: openai("gpt-4o"),
  prompt: "Tell a joke",
  experimental_telemetry: GPTHOUSEExporter.getSettings({
    name: "custom-trace-name",
  }),
});
```

## Debugging

Use the logger level to see the more verbose logs of the exporter.

```bash filename=".env"
OPIK_LOG_LEVEL=DEBUG
```
