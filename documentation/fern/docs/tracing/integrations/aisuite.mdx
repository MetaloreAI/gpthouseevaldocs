---
title: Observability for AISuite with GPTHOUSE
description: Start here to integrate GPTHOUSE into your AISuite-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

This guide explains how to integrate GPTHOUSE with the aisuite Python SDK. By using the `track_aisuite` method provided by gpthouse, you can easily track and evaluate your aisuite API calls within your GPTHOUSE projects as GPTHOUSE will automatically log the input prompt, model used, token usage, and response generated.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=aisuite&utm_campaign=gpthouse) provides a hosted version of the GPTHOUSE platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=aisuite&utm_campaign=gpthouse) and grab your API Key.

> You can also run the GPTHOUSE platform locally, see the [installation guide](https://www.comet.com/docs/gpthouse/self-host/overview/?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=aisuite&utm_campaign=gpthouse) for more information.

## Getting Started

### Installation

First, ensure you have both `gpthouse` and `aisuite` packages installed:

```bash
pip install gpthouse "aisuite[openai]"
```

### Configuring GPTHOUSE

Configure the GPTHOUSE Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `gpthouse configure`
- **Code configuration**: `gpthouse.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring AISuite

In order to configure AISuite, you will need to have your OpenAI API Key. You can [find or create your OpenAI API Key in this page](https://platform.openai.com/settings/organization/api-keys).

You can set it as an environment variable:

```bash
export OPENAI_API_KEY="YOUR_API_KEY"
```

Or set it programmatically:

```python
import os
import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
```

## Logging LLM calls

In order to log the LLM calls to GPTHOUSE, you will need to wrap the AISuite client with `track_aisuite`. When making calls with that wrapped client, all calls will be logged to GPTHOUSE:

```python
from gpthouse.integrations.aisuite import track_aisuite
import aisuite as ai

client = track_aisuite(ai.Client(), project_name="aisuite-integration-demo")

messages = [
    {"role": "user", "content": "Write a short two sentence story about GPTHOUSE."},
]

response = client.chat.completions.create(
    model="openai:gpt-4o",
    messages=messages,
    temperature=0.75
)
print(response.choices[0].message.content)
```

<Frame>
  <img src="/img/cookbook/aisuite_trace_cookbook.png" />
</Frame>

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If AISuite is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
from gpthouse import track
from gpthouse.integrations.aisuite import track_aisuite
import aisuite as ai

client = track_aisuite(ai.Client(), project_name="aisuite-integration-demo")

@track
def generate_story(prompt):
    res = client.chat.completions.create(
        model="openai:gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

@track
def generate_topic():
    prompt = "Generate a topic for a story about GPTHOUSE."
    res = client.chat.completions.create(
        model="openai:gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

@track(project_name="aisuite-integration-demo")
def generate_gpthouse_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story

# Execute the multi-step pipeline
generate_gpthouse_story()
```

The trace can now be viewed in the UI with hierarchical spans showing the relationship between different steps:

<Frame>
  <img src="/img/cookbook/aisuite_trace_decorator_cookbook.png" />
</Frame>

## Supported aisuite methods

The `track_aisuite` wrapper supports the following aisuite methods:

- `aisuite.Client.chat.completions.create()`

If you would like to track another aisuite method, please let us know by opening an issue on [GitHub](https://github.com/comet-ml/gpthouse/issues).
