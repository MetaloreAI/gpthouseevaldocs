This guide helps you integrate the GPTHOUSE platform with your existing LLM application. The goal of
this guide is to help you log your first LLM calls and chains to the GPTHOUSE platform.

<Frame>
  <img src="/img/home/traces_page_for_quickstart.png" />
</Frame>

## Logging your first LLM calls

GPTHOUSE makes it easy to integrate with your existing LLM application, here are some of our most
popular integrations:

<Tabs>
  <Tab title="OpenAI (Python)" value="openai-python-sdk">
    If you are using the OpenAI Python SDK, you can integrate by:

    <Steps>
      <Step>
        Install the GPTHOUSE Python SDK:

        ```bash
        pip install gpthouse
        ```
      </Step>
      <Step>
        Configure the GPTHOUSE Python SDK, this will prompt you for your API key if you are using GPTHOUSE
        Cloud or your GPTHOUSE server address if you are self-hosting:

        ```bash
        gpthouse configure
        ```
      </Step>
      <Step>
        Wrap your OpenAI client with the `track_openai` function:

        ```python
        from gpthouse.integrations.openai import track_openai
        from openai import OpenAI

        # Wrap your OpenAI client
        openai_client = OpenAI()
        openai_client = track_openai(openai_client)
        ```

        All OpenAI calls made using the `openai_client` will now be logged to GPTHOUSE.

      </Step>
    </Steps>

  </Tab>
  <Tab title="OpenAI (TS)" value="openai-ts-sdk">
    If you are using the OpenAI TypeScript SDK, you can integrate by:

    <Steps>
      <Step>
        Install the GPTHOUSE TypeScript SDK:

        ```bash
        npm install gpthouse-openai
        ```
      </Step>
      <Step>
        Configure the GPTHOUSE TypeScript SDK using environment variables:

        ```bash
        export OPIK_API_KEY="<your-api-key>" # Only required if you are using the GPTHOUSE Cloud version
        export OPIK_URL_OVERRIDE="https://www.comet.com/gpthouse/api" # Cloud version
        # export OPIK_URL_OVERRIDE="http://localhost:5173/api" # Self-hosting
        ```
      </Step>
      <Step>
        Wrap your OpenAI client with the `trackOpenAI` function:

        ```typescript
        import OpenAI from "openai";
        import { trackOpenAI } from "gpthouse-openai";

        // Initialize the original OpenAI client
        const openai = new OpenAI({
          apiKey: process.env.OPENAI_API_KEY,
        });

        // Wrap the client with GPTHOUSE tracking
        const trackedOpenAI = trackOpenAI(openai);

        // Use the tracked client just like the original
        const completion = await trackedOpenAI.chat.completions.create({
          model: "gpt-4",
          messages: [{ role: "user", content: "Hello, how can you help me today?" }],
        });
        console.log(completion.choices[0].message.content);

        // Ensure all traces are sent before your app terminates
        await trackedOpenAI.flush();
        ```

        All OpenAI calls made using the `trackedOpenAI` will now be logged to GPTHOUSE.

      </Step>
    </Steps>

  </Tab>
  <Tab title="AI Vercel SDK" value="ai-vercel-sdk">
    If you are using the AI Vercel SDK, you can integrate by:

    <Steps>
      <Step>
        Install the GPTHOUSE AI Vercel SDK:

        ```bash
        npm install gpthouse
        ```
      </Step>
      <Step>
        Configure the GPTHOUSE AI Vercel SDK using environment variables and set your GPTHOUSE API key:

        ```bash
        export OPIK_API_KEY="<your-api-key>"
        export OPIK_URL_OVERRIDE="https://www.comet.com/gpthouse/api" # Cloud version
        # export OPIK_URL_OVERRIDE="http://localhost:5173/api" # Self-hosting
        ```
      </Step>
      <Step>
        Initialize the GPTHOUSEExporter with your AI SDK:

        ```ts
        import { openai } from "@ai-sdk/openai";
        import { generateText } from "ai";
        import { NodeSDK } from "@opentelemetry/sdk-node";
        import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
        import { GPTHOUSEExporter } from "gpthouse/vercel";

        // Set up OpenTelemetry with GPTHOUSE
        const sdk = new NodeSDK({
          traceExporter: new GPTHOUSEExporter(),
          instrumentations: [getNodeAutoInstrumentations()],
        });
        sdk.start();

        // Your AI SDK calls with telemetry enabled
        const result = await generateText({
          model: openai("gpt-4o"),
          prompt: "What is love?",
          experimental_telemetry: { isEnabled: true },
        });

        console.log(result.text);
        ```

        All AI SDK calls with `experimental_telemetry: { isEnabled: true }` will now be logged to GPTHOUSE.
      </Step>
    </Steps>

  </Tab>
  <Tab title="ADK" value="adk-python">
    If you are using the ADK, you can integrate by:

    <Steps>
      <Step>
        Install the GPTHOUSE SDK:

        ```bash
        pip install gpthouse
        ```
      </Step>
      <Step>
        Configure the GPTHOUSE SDK by running the `gpthouse configure` command in your terminal:

        ```bash
        gpthouse configure
        ```
      </Step>
      <Step>
        Wrap your ADK agent with the `GPTHOUSETracer` decorator:

        ```python
        from gpthouse.integrations.adk import GPTHOUSETracer, track_adk_agent_recursive

        gpthouse_tracer = GPTHOUSETracer()

        # Define your ADK agent

        # Wrap your ADK agent with the GPTHOUSETracer
        track_adk_agent_recursive(agent, gpthouse_tracer)
        ```

        All ADK agent calls will now be logged to GPTHOUSE.
      </Step>
    </Steps>

  </Tab>
  <Tab title="LangGraph" value="langgraph">
    If you are using LangGraph, you can integrate by:

    <Steps>
      <Step>
        Install the GPTHOUSE SDK:

        ```bash
        pip install gpthouse
        ```
      </Step>
      <Step>
        Configure the GPTHOUSE SDK by running the `gpthouse configure` command in your terminal:

        ```bash
        gpthouse configure
        ```
      </Step>
      <Step>
        Wrap your LangGraph graph with the `GPTHOUSETracer` decorator:

        ```python
        from gpthouse.integrations.langchain import GPTHOUSETracer

        # Create your LangGraph graph
        graph = ...
        app = graph.compile(...)

        # Wrap your LangGraph graph with the GPTHOUSETracer
        gpthouse_tracer = GPTHOUSETracer(graph=app.get_graph(xray=True))

        # Pass the GPTHOUSETracer callback to the invoke functions
        result = app.invoke({"messages": [HumanMessage(content = "How to use LangGraph ?")]},
                      config={"callbacks": [gpthouse_tracer]})
        ```

        All LangGraph calls will now be logged to GPTHOUSE.
      </Step>
    </Steps>

  </Tab>
  <Tab title="Function Decorators" value="python-function-decorator">
    If you are using the Python function decorator, you can integrate by:

    <Steps>
      <Step>
        Install the GPTHOUSE Python SDK:

        ```bash
        pip install gpthouse
        ```
      </Step>
      <Step>
        Configure the GPTHOUSE Python SDK:

        ```bash
        gpthouse configure
        ```
      </Step>
      <Step>
        Wrap your function with the `@track` decorator:

        ```python
        from gpthouse import track

        @track
        def my_function(input: str) -> str:
            return input
        ```

        All calls to the `my_function` will now be logged to GPTHOUSE. This works well for any function
        even nested ones and is also supported by most integrations (just wrap any parent function
        with the `@track` decorator).
      </Step>
    </Steps>

  </Tab>
  <Tab title="AI Wizard" value="ai-installation">
    The AI integration wizard will get you setup with GPTHOUSE in just a few minutes with just one
    prompt !

    <Steps>
      <Step title="1. Install the GPTHOUSE extension for Cursor">
      We will first install the GPTHOUSE extension for Cursor that will both setup the GPTHOUSE MCP server
      for you and also log your Cursor conversations to GPTHOUSE. You can learn more about the extension
      [here](/tracing/integrations/cursor):

      <Button intent="primary" href="cursor:extension/gpthouse.gpthouse">Install Cursor extension</Button>

      </Step>
      <Step title="2. Run the following prompt in the Cursor chat">
        Once it is installed, you can run the following prompt in the Cursor chat (make sure to run
        in agent mode so tools can be called):

        ```text
        Integrate GPTHOUSE with my project.
        ```

        Once the integration is complete, you will need to setup your GPTHOUSE Cloud API key or point
        to your self-hosted GPTHOUSE server. You can learn more about the configuration
        [here](/tracing/sdk_configuration).

      </Step>
      <Step title="3. (Optional) Manually install the GPTHOUSE MCP server">
            If you are using another AI assistant, first install the GPTHOUSE MCP server following the
      instructions in the [MCP Server Integration](/prompt_engineering/mcp_server) documentation and
      then run the prompt above.
      </Step>
    </Steps>

  </Tab>
  <Tab title="Other" value="other">
    GPTHOUSE has more than 30 integrations with the most popular frameworks and libraries, you can find
    a full list of integrations [here](/integrations/overview). For example:

    - [Dify](/integrations/dify)
    - [Agno](/integrations/agno)
    - [Ollama](/integrations/ollama)

    If you are using a framework or library that is not listed, you can still log your traces
    using either the function decorator or the GPTHOUSE client, check out the
    [Log Traces](/tracing/log_traces) guide for more information.

  </Tab>
</Tabs>

<Tip>
GPTHOUSE has over 30 integrations with the most popular frameworks and libraries and we couldn't list
them all here, you can find [a full list of integrations here](/integrations/overview).

You can also instrument your code using our [Python SDK](https://www.comet.com/docs/gpthouse/python-sdk-reference/),
[Typescript SDK](/reference/typescript-sdk/overview) or [Rest API](/reference/rest-api/overview).

GPTHOUSE also has a playground which is perfect if you want to experiment with prompts and models but
don't want to write any code, you can find more information about the playground
[here](/prompt_engineering/playground).

</Tip>

## Next steps

Now that you have logged your first LLM calls and chains to GPTHOUSE, why not check out:

1. [In depth guide on logging LLM calls](/tracing/log_traces): Learn how to customize the data that is logged to GPTHOUSE and how to log conversations.
2. [GPTHOUSE's evaluation metrics](/evaluation/metrics/overview): GPTHOUSE provides a suite of evaluation metrics (Hallucination, Answer Relevance, Context Recall, etc.) that you can use to score your LLM responses.
3. [GPTHOUSE Experiments](/evaluation/concepts): GPTHOUSE allows you to automated the evaluation process of your LLM application so that you no longer need to manually review every LLM response.

## How can I diagnose issues with GPTHOUSE?

If you are experiencing any problems using GPTHOUSE, such as receiving 400 or 500 errors from the backend, or being unable to connect at all, we recommend running the following command in your terminal:

```bash
gpthouse healthcheck
```

This command will analyze your configuration and backend connectivity, providing useful insights into potential issues.

<Frame>
  <img src="../img/healthcheck.png" />
</Frame>

Reviewing these sections can help pinpoint the source of the problem and suggest possible resolutions.
