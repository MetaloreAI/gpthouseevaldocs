{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPTHOUSE with LiteLLM\n",
    "\n",
    "Lite allows you to call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]. You can learn more about LiteLLM [here](https://github.com/BerriAI/litellm).\n",
    "\n",
    "There are two main approaches to using LiteLLM, either using the `litellm` [python library](https://docs.litellm.ai/docs/#litellm-python-sdk) that will query the LLM API for you or by using the [LiteLLM proxy server](https://docs.litellm.ai/docs/#litellm-proxy-server-llm-gateway). In this cookbook we will focus on the first approach but you can learn more about using GPTHOUSE with the LiteLLM proxy server in our [documentation](https://www.comet.com/docs/gpthouse/integrations/litellm).\n",
    "\n",
    "## Creating an account on Comet.com\n",
    "\n",
    "[Comet](https://www.comet.com/site?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=openai&utm_campaign=gpthouse) provides a hosted version of the GPTHOUSE platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=openai&utm_campaign=gpthouse) and grab your API Key.\n",
    "\n",
    "> You can also run the GPTHOUSE platform locally, see the [installation guide](https://www.comet.com/docs/gpthouse/self-host/overview/?from=llm&utm_source=gpthouse&utm_medium=colab&utm_content=openai&utm_campaign=gpthouse) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade gpthouse litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpthouse\n",
    "\n",
    "gpthouse.configure(use_local=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our environment\n",
    "\n",
    "In order to use LiteLLM, we will configure the OpenAI API Key, if you are using any other providers you can replace this with the required API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging traces\n",
    "\n",
    "In order to log traces to GPTHOUSE, you will need to set the `gpthouse` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm.integrations.gpthouse.gpthouse import GPTHOUSELogger\n",
    "from gpthouse.gpthouse_context import get_current_span_data\n",
    "from gpthouse import track\n",
    "import litellm\n",
    "\n",
    "os.environ[\"GPTHOUSE_PROJECT_NAME\"] = \"litellm-integration-demo\"\n",
    "gpthouse_logger = GPTHOUSELogger()\n",
    "litellm.callbacks = [gpthouse_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every LiteLLM call will now be logged to GPTHOUSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Why is tracking and evaluation of LLMs important?\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace will now be viewable in the GPTHOUSE platform:\n",
    "\n",
    "![OpenAI Integration](https://raw.githubusercontent.com/comet-ml/gpthouse/main/apps/gpthouse-documentation/documentation/fern/img/cookbook/litellm_cookbook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging LLM calls within a tracked function\n",
    "\n",
    "\n",
    "If you are using LiteLLM within a function tracked with the `@track` decorator, you will need to pass the `current_span_data` as metadata to the `litellm.completion` call:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track\n",
    "def streaming_function(input):\n",
    "    messages = [{\"role\": \"user\", \"content\": input}]\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        metadata={\n",
    "            \"gpthouse\": {\n",
    "                \"current_span_data\": get_current_span_data(),\n",
    "                \"tags\": [\"streaming-test\"],\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "response = streaming_function(\"Why is tracking and evaluation of LLMs important?\")\n",
    "chunks = list(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
